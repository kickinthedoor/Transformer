{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d59dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPUs available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6f327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng_-french.csv', 'r', encoding='utf-8') as f, \\\n",
    "     open('english.txt', 'w', encoding='utf-8') as en, \\\n",
    "     open('french.txt', 'w', encoding='utf-8') as fr:\n",
    "    for line in f:\n",
    "        clean_line = \" \".join(line.strip().split())\n",
    "        english_sentence, french_sentence = clean_line.strip().split(',', 1)\n",
    "        english_sentence = re.sub('([.,!?()\"])', r' \\1 ', english_sentence)\n",
    "        english_sentence = re.sub('\\s{2,}', ' ', english_sentence)\n",
    "        french_sentence = re.sub('([.,!?()\"])', r' \\1 ', french_sentence)\n",
    "        french_sentence = re.sub('\\s{2,}', ' ', french_sentence)\n",
    "        fr.write('' + french_sentence + '\\n')\n",
    "        en.write('' + english_sentence + '\\n')\n",
    "    f.close()\n",
    "    en.close()\n",
    "    fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4dd1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 8168, 152, 2], [1, 19397, 152, 2], [1, 19807, 152, 2], [1, 547, 36, 2], [1, 31927, 3, 31930, 1451, 152, 2], [1, 1027, 1174, 152, 2], [1, 31927, 3, 11, 31944, 1167, 152, 2], [1, 13783, 4, 2], [1, 31927, 3, 31930, 4360, 152, 2], [1, 6759, 282, 152, 2], [1, 1129, 31935, 3, 919, 31953, 1146, 152, 2], [1, 3646, 152, 2], [1, 3710, 152, 2], [1, 15864, 4, 2], [1, 6730, 4, 2]]\n",
      "[[1, 6330, 4, 2], [1, 5518, 412, 2], [1, 5518, 412, 2], [1, 421, 33, 2], [1, 6350, 412, 2], [1, 8180, 412, 2], [1, 2438, 412, 2], [1, 8737, 4, 2], [1, 1052, 412, 2], [1, 1052, 412, 2], [1, 1052, 412, 2], [1, 2134, 412, 2], [1, 2134, 412, 2], [1, 1062, 83, 4, 2], [1, 1062, 83, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "# using french as the source language and english as the target\n",
    "spm.SentencePieceTrainer.Train(input='french.txt',pad_id=0,unk_id=3,model_prefix='sourcemodel',vocab_size=32000,character_coverage=0.9995,model_type='bpe',control_symbols=['<s>','</s>'])\n",
    "spm.SentencePieceTrainer.Train(input='english.txt',pad_id=0,unk_id=3,model_prefix='targetmodel',vocab_size=32000,character_coverage=0.9995,model_type='bpe',control_symbols=['<s>','</s>'])\n",
    "\n",
    "# Load the trained models\n",
    "sp_source = spm.SentencePieceProcessor()\n",
    "sp_source.Load('sourcemodel.model')\n",
    "\n",
    "sp_target = spm.SentencePieceProcessor()\n",
    "sp_target.Load('targetmodel.model')\n",
    "\n",
    "sos_fr = sp_source.piece_to_id('<s>')\n",
    "eos_fr = sp_source.piece_to_id('</s>')\n",
    "\n",
    "sos_en = sp_target.piece_to_id('<s>')\n",
    "eos_en = sp_target.piece_to_id('</s>')\n",
    "\n",
    "# Tokenize example\n",
    "tokens_fr = []\n",
    "french = open('french.txt', 'r')\n",
    "Lines = french.readlines()\n",
    "for line in Lines:\n",
    "    tokens_fr.append([sos_fr] + sp_source.EncodeAsIds(line) + [eos_fr])\n",
    "\n",
    "\n",
    "tokens_en = []\n",
    "english = open('english.txt', 'r')\n",
    "Lines = english.readlines()\n",
    "for line in Lines:\n",
    "    tokens_en.append([sos_en] + sp_target.EncodeAsIds(line) + [eos_en])\n",
    "\n",
    "\n",
    "tokens_fr = tokens_fr[1:]\n",
    "tokens_en = tokens_en[1:]\n",
    "\n",
    "print(tokens_fr[0:15])\n",
    "print(tokens_en[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f107c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence length: 40\n",
      "Average sequence length: 9.64\n",
      "90th percentile sequence length: 13.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the lengths of each sequence\n",
    "sequence_lengths = [len(seq) for seq in tokens_en]\n",
    "\n",
    "# Calculate the statistics\n",
    "longest_sequence = max(sequence_lengths)\n",
    "average_sequence_length = np.mean(sequence_lengths)\n",
    "percentile_90_length = np.percentile(sequence_lengths, 90)\n",
    "\n",
    "# Output the stats\n",
    "print(f\"Longest sequence length: {longest_sequence}\")\n",
    "print(f\"Average sequence length: {average_sequence_length:.2f}\")\n",
    "print(f\"90th percentile sequence length: {percentile_90_length}\")\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(sequence_lengths, bins=range(min(sequence_lengths), max(sequence_lengths) + 1))\n",
    "plt.title('Histogram of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b98a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140496\n",
      "17562\n",
      "17563\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing (80-20 split, for example)\n",
    "en_train, en_temp, fr_train, fr_temp = train_test_split(tokens_en, tokens_fr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temp data again into validation and test sets (50-50 split)\n",
    "en_val, en_test, fr_val, fr_test = train_test_split(en_temp, fr_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "buffer_train = len(en_train)\n",
    "buffer_val = len(en_val)\n",
    "buffer_test = len(en_test)\n",
    "print(buffer_train)\n",
    "print(buffer_val)\n",
    "print(buffer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2fd86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(padded_sequences):\n",
    "    mask = (padded_sequences == 0).int()  \n",
    "    return mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_data, target_data):\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.source_data[idx]\n",
    "        target = self.target_data[idx]\n",
    "        source_tensor = torch.tensor(source, dtype=torch.int32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.int32)\n",
    "        return source_tensor, target_tensor\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inp, tar = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    inp_padded = pad_sequence(inp, batch_first=True, padding_value=0)\n",
    "    tar_padded = pad_sequence(tar, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Create masks for the padded sequences\n",
    "    mask_inp = create_masks(inp_padded)\n",
    "    mask_tar = create_masks(tar_padded)\n",
    "    \n",
    "    return (inp_padded, mask_inp), (tar_padded, mask_tar)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = TranslationDataset(fr_train, en_train)\n",
    "val_dataset = TranslationDataset(fr_val, en_val)\n",
    "test_dataset = TranslationDataset(fr_test, en_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07bf0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Source sequence: tensor([   1,  547,   16,  581,   42,    3,   55, 7670,   36,    2,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)\n",
      "Source mask: tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.int32)\n",
      "Target sequence: tensor([   1,  421, 1148,   26, 1046,   33,    2,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0], dtype=torch.int32)\n",
      "Target mask: tensor([[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "       dtype=torch.int32)\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "Source sequence: tensor([    1,    63,    23, 31944, 31961,    16,    37,   584, 31953,  4679,\n",
      "           41,   121,  1493,   135,   100,   328,     4,     2,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0], dtype=torch.int32)\n",
      "Source mask: tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.int32)\n",
      "Target sequence: tensor([    1,   304, 31967, 31952,   111,   337,     9,   112,    64,    89,\n",
      "           16,     4,     2,     0,     0,     0,     0,     0],\n",
      "       dtype=torch.int32)\n",
      "Target mask: tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]],\n",
      "       dtype=torch.int32)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter\n",
    "counter = 0\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for (source_batch, source_masks), (target_batch, target_masks) in train_loader:\n",
    "    # Flatten the batches into individual samples\n",
    "    for i in range(len(source_batch)):\n",
    "        print(f\"Sample {counter + 1}:\")\n",
    "        print(\"Source sequence:\", source_batch[i])\n",
    "        print(\"Source mask:\", source_masks[i])\n",
    "        print(\"Target sequence:\", target_batch[i])\n",
    "        print(\"Target mask:\", target_masks[i])\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        # Stop after the first 5 samples\n",
    "        if counter >= 2:\n",
    "            break\n",
    "    if counter >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9273a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(max_len, d_model):\n",
    "    # Create a matrix to store the positional encodings\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)]\n",
    "        for pos in range(max_len)\n",
    "    ])\n",
    "\n",
    "    # Apply the sine to the even indices and the cosine to the odd indices\n",
    "    pos_enc[:, 0::2] = np.sin(pos_enc[:, 0::2])\n",
    "    pos_enc[:, 1::2] = np.cos(pos_enc[:, 1::2])\n",
    "\n",
    "    pos_enc = pos_enc[np.newaxis, ...]  # Add a new dimension for the batch size\n",
    "    return torch.from_numpy(pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f75dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    The mask has different shapes depending on its type (padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "    - q: query shape == (..., seq_len_q, depth)\n",
    "    - k: key shape == (..., seq_len_k, depth)\n",
    "    - v: value shape == (..., seq_len_v, depth_v)\n",
    "    - mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "    - output, attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the dot product\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # Scale the dot product\n",
    "    d_k = k.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    # Apply the mask if provided\n",
    "    if mask is not None:\n",
    "        \n",
    "        mask = mask.float()\n",
    "        \n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # Softmax to get the attention weights\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "\n",
    "    # Multiply the attention weights with the value matrix\n",
    "    output = torch.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cefec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    # Create a lower triangular matrix and subtract it from 1.\n",
    "    # This will produce a matrix with zeros in its lower triangle and ones elsewhere.\n",
    "    mask = 1 - torch.triu(torch.ones(size, size))\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c8299f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_match(seq1, seq2, padding_value=0):\n",
    "    len_seq1 = seq1.shape[0]\n",
    "    len_seq2 = seq2.shape[0]\n",
    "\n",
    "    padding_seq1 = max(len_seq2 - len_seq1, 0)\n",
    "    padding_seq2 = max(len_seq1 - len_seq2, 0)\n",
    "\n",
    "    seq1_padded = F.pad(seq1, (0, padding_seq1), value=padding_value)\n",
    "    seq2_padded = F.pad(seq2, (0, padding_seq2), value=padding_value)\n",
    "\n",
    "    return seq1_padded, seq2_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f1c6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model,d_model)\n",
    "        self.wk = nn.Linear(d_model,d_model)\n",
    "        self.wv = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.fc = nn.Linear(d_model,d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Use the previously defined scaled_dot_product_attention function\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)\n",
    "        concat_attention = scaled_attention.contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.fc(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46220042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, dff)  # dff: dimensionality of feed-forward inner layer\n",
    "        self.fc2 = nn.Linear(dff, dff)  \n",
    "        self.fc3 = nn.Linear(dff,d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "902c3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)  \n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model,eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model,eps=1e-6)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # Self attention\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = get_positional_encoding(maximum_position_encoding, self.d_model).float() \n",
    "        \n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, dff, rate) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Adding embedding and position encoding\n",
    "        x = self.embedding(x)  \n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :seq_length, :]\n",
    "        x += pos_encoding\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "            \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed8e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Self attention\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Cross attention with encoder's output\n",
    "\n",
    "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model,eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model,eps=1e-6)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model,eps=1e-6)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, enc_output, combined_mask, enc_padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)  # Self attention\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, enc_padding_mask)  # Cross attention\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = get_positional_encoding(maximum_position_encoding, d_model).float()\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, dff, rate) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, enc_padding_mask, dec_padding_mask):\n",
    "        \n",
    "        look_ahead_mask = look_ahead_mask.float()\n",
    "        dec_padding_mask = dec_padding_mask.float()\n",
    "        combined_mask = torch.max(look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model,dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :seq_length, :]\n",
    "        x += pos_encoding\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, look_ahead_mask, enc_padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        return x, attention_weights  # x.shape == (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c2acdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model,target_vocab_size)\n",
    "    \n",
    "    def forward(self, inp, tar, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, look_ahead_mask, enc_padding_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f021a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(_LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=15000,last_epoch=-1):\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "        super(CustomSchedule, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        \n",
    "    def get_lr(self):\n",
    "        step = max(1, self.last_epoch)\n",
    "        arg1 = 1.0 / math.sqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        scale = math.sqrt(self.d_model) * min(arg1, arg2)\n",
    "        return [base_lr * scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f8df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "input_vocab_size = 32000\n",
    "target_vocab_size = 32000\n",
    "pe_input = 500\n",
    "pe_target = 500\n",
    "rate=0.01\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=rate) \n",
    "\n",
    "\n",
    "optimizer = optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "learning_rate = CustomSchedule(optimizer,d_model)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename=\"checkpoint.pth.tar\"):\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint '{filename}'\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        transformer.load_state_dict(checkpoint['transformer_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        learning_rate.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(f\"Checkpoint loaded successfully from '{filename}'\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "611a7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = (real != 0).float()\n",
    "    \n",
    "    # Compute the loss (cross-entropy loss)\n",
    "    loss_ = F.cross_entropy(pred, real, reduction='none')\n",
    "    \n",
    "    # Apply the mask to the loss\n",
    "    loss_ = loss_ * mask\n",
    "    \n",
    "    # Calculate the average loss, ignoring the padding\n",
    "    return loss_.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "082a5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_smoothing(real, pred, smoothing_rate=0.1):\n",
    "    num_classes = pred.size(-1)\n",
    "\n",
    "    real = real.long()\n",
    "\n",
    "    # Convert 'real' to a one-hot encoding\n",
    "    real_one_hot = F.one_hot(real, num_classes=num_classes).float()\n",
    "\n",
    "    # Apply label smoothing\n",
    "    real_smoothed = real_one_hot * (1.0 - smoothing_rate) + (smoothing_rate / num_classes)\n",
    "\n",
    "    # Compute the cross-entropy loss using smoothed labels\n",
    "    cross_entropy = torch.sum(-real_smoothed * F.log_softmax(pred, dim=-1), dim=-1)\n",
    "\n",
    "    mask = (real != 0).float()\n",
    "\n",
    "    loss_ = cross_entropy * mask\n",
    "\n",
    "    # Calculate the average loss considering the mask\n",
    "    return torch.sum(loss_) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "179cb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_translation_accuracy(predictions, references):\n",
    "    \n",
    "    predictions = torch.tensor(predictions)\n",
    "    references = torch.tensor(references)\n",
    "\n",
    "    # Calculate the total number of tokens (excluding padding)\n",
    "    total_tokens = torch.sum(references != 0)  # Assuming 0 is the padding token\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = torch.sum((predictions == references) & (references != 0))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions.item() / total_tokens.item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a8bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, targets, padding_value=0):\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    mask = targets != padding_value\n",
    "    masked_predictions = predictions[mask]\n",
    "    masked_targets = targets[mask]\n",
    "    correct_predictions = (masked_predictions == masked_targets).float()\n",
    "    accuracy = correct_predictions.mean().item()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aad08290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(transformer, val_loader, n_translations=5):\n",
    "    transformer.eval()\n",
    "    generated_sequences = []\n",
    "    actual_sequences = []\n",
    "    samples = 0\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, ((inp, mask_inp), (tar, mask_tar)) in enumerate(val_loader):\n",
    "            generated_sequence, avg_loss = generate_output(transformer, inp, tar, mask_inp)\n",
    "            generated_list = generated_sequence.squeeze(0).cpu().tolist()\n",
    "            generated_sentence = sp_target.DecodeIds(generated_list)\n",
    "            generated_sequences.append(generated_sentence.split())\n",
    "            \n",
    "            actual_sequence = tar[:, 1:]\n",
    "            actual_list = actual_sequence.squeeze(0).cpu().tolist()\n",
    "            actual_sentence = sp_target.DecodeIds([int(token) for sublist in actual_list for token in sublist])\n",
    "            actual_sequences.append([actual_sentence.split()])\n",
    "            \n",
    "            samples += 1\n",
    "            a_pad, g_pad = pad_to_match(tar.squeeze(), generated_sequence)\n",
    "            accuracy = calculate_translation_accuracy(g_pad, a_pad)\n",
    "            acc += accuracy\n",
    "            loss += avg_loss\n",
    "            \n",
    "            if samples < n_translations:\n",
    "                print(\"src_id: \" + str(generated_list) + \"\\ntar_id: \" + str([1] + actual_list[0]))\n",
    "                print(\"Pred: \" + generated_sentence + \"\\nActual: \" + actual_sentence + \"\\n\")\n",
    "            \n",
    "            if samples == 10000:\n",
    "                break\n",
    "    \n",
    "    print(f\"Translation Accuracy: {acc / samples * 100:.2f}%\")\n",
    "    print(\"Translation Loss: \" + str(loss / samples))\n",
    "    \n",
    "    average_bleu = corpus_bleu(actual_sequences, generated_sequences)\n",
    "    \n",
    "    return average_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e82896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(transformer, input_sequence, tar, input_mask, start_token=1, end_token=2, max_len=40):\n",
    "    transformer.eval()\n",
    "    batch_size = input_sequence.size(0)\n",
    "    output = torch.full((batch_size, 1), start_token, dtype=torch.long).to(input_sequence.device)\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    eos_flags = torch.zeros(batch_size, dtype=torch.bool, device=input_sequence.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len):\n",
    "            look_ahead_mask = create_look_ahead_mask(output.size(1)).to(input_sequence.device)\n",
    "            target_padding_mask = (output == 0).unsqueeze(1).unsqueeze(2).int()\n",
    "            \n",
    "            predictions, _ = transformer(input_sequence, output, input_mask, look_ahead_mask, target_padding_mask)\n",
    "            loss = loss_function_smoothing(tar[:, min(i + 1, tar.size(1) - 1)], predictions[:, -1, :])\n",
    "            total_loss += loss.item()\n",
    "            total_count += 1\n",
    "            \n",
    "            predictions = predictions[:, -1:, :]\n",
    "            predicted_id = torch.argmax(predictions, dim=-1)\n",
    "            \n",
    "            output = torch.cat([output, predicted_id], dim=-1)\n",
    "            \n",
    "            # Check if all sequences in the batch have predicted the end token\n",
    "            eos_flags |= (predicted_id.squeeze() == end_token)\n",
    "            if eos_flags.all():\n",
    "                break\n",
    "    \n",
    "    return output, total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1942c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(transformer, optimizer, scheduler, inp, tar, mask_inp, mask_tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    mask_tar_inp = mask_tar[:, :, :, :-1]\n",
    "    mask_tar_real = mask_tar[:, :, :, 1:]\n",
    "    seq_len = tar_inp.size(1)\n",
    "    look_ahead_mask = create_look_ahead_mask(seq_len).to(inp.device)\n",
    "    transformer.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions, _ = transformer(inp, tar_inp, mask_inp, look_ahead_mask, mask_tar_inp)\n",
    "    loss = loss_function_smoothing(tar_real, predictions, smoothing_rate=0.1)\n",
    "    accuracy = calculate_accuracy(predictions, tar_real)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss.item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "096a4cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 19:25:11\n",
      "\n",
      "\n",
      "Epoch 1 Batch 0 Loss 10.5375 Accuracy 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and setup\n",
    "EPOCHS = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the transformer, optimizer, and scheduler\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=rate)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CustomSchedule(optimizer, d_model)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Logging setup\n",
    "loss_log = open(\"loss.txt\", \"w\")\n",
    "acc_log = open(\"accuracy.txt\", \"w\")\n",
    "bleu_log = open(\"bleu.txt\", \"w\")\n",
    "\n",
    "# Start training\n",
    "start = datetime.now()\n",
    "start_time = start.strftime(\"%H:%M:%S\")\n",
    "print(\"Start Time =\", start_time)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    iteration = 0\n",
    "    print(\"\\n\")\n",
    "    for batch, ((inp, mask_inp), (tar, mask_tar)) in enumerate(train_loader):\n",
    "        batch_loss, accuracy = train_step(transformer, optimizer, scheduler, inp, tar, mask_inp, mask_tar)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += accuracy\n",
    "        iteration += 1\n",
    "        loss_log.write(f\"{batch_loss}\\n\")\n",
    "        acc_log.write(f\"{accuracy}\\n\")\n",
    "        if batch % 50 == 0:  # Print the batch loss every 50 batches\n",
    "            print(f\"Epoch {epoch + 1} Batch {batch} Loss {batch_loss:.4f} Accuracy {accuracy:.4f}\")\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        avg_bleu = validate(transformer, val_loader)\n",
    "        bleu_log.write(f\"{avg_bleu}\\n\")\n",
    "        print(f\"Average Validation BLEU Score for epoch {epoch + 1} is {avg_bleu:.4f}\")\n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"./checkpoints/train/epoch_{epoch + 1}.pth\"\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'transformer_state_dict': transformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': total_loss / iteration,\n",
    "            'accuracy': total_accuracy / iteration,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Saving checkpoint for epoch {epoch + 1} at {checkpoint_path}')\n",
    "    print(f\"Epoch {epoch + 1} Loss {total_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} Average Loss {total_loss / iteration:.4f} Average Accuracy {total_accuracy / iteration:.4f}\")\n",
    "    loss_log.write(f\"Avg: {total_loss / iteration}\\n\")\n",
    "    acc_log.write(f\"Avg: {total_accuracy / iteration}\\n\")\n",
    "\n",
    "loss_log.close()\n",
    "acc_log.close()\n",
    "bleu_log.close()\n",
    "\n",
    "end = datetime.now()\n",
    "end_time = end.strftime(\"%H:%M:%S\")\n",
    "print(\"End Time =\", end_time)\n",
    "delta = end - start\n",
    "print(f\"Time difference is {delta.total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263c99c",
   "metadata": {},
   "source": [
    "Implement GPU usage, model saving and attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd5858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
